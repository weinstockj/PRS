Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Job stats:
job             count    min threads    max threads
------------  -------  -------------  -------------
train_region        1              2              2
total               1              2              2

Select jobs to execute...

[Tue Feb 13 20:39:47 2024]
rule train_region:
    input: data/ld_blocks/sm_run/chr21.txt, data/ld_blocks/sm_run/chr22.txt
    jobid: 0
    threads: 2
    resources: tmpdir=/tmp, mem=190G, partition=parallel, time=40:00:00

[Tue Feb 13 20:39:47 2024]
Error in rule train_region:
    jobid: 0
    shell:
        
        while IFS= read -r chunk; do
            ~/data-abattle4/april/programs/julia-1.9.4/bin/julia --project=. -t 2 -e 'using Revise, PRSFNN; PRSFNN.main("{$chunk}", "/data/abattle4/april/hi_julia/annotations/ccre/celltypes", "/data/abattle4/jweins17/LD_REF_PANEL/output/bcf")'
        done < data/ld_blocks/sm_run/chr21.txt data/ld_blocks/sm_run/chr22.txt
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-02-13T203937.138758.snakemake.log
